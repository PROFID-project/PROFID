
# =======================================================================================
## Study 5_UMBIZO_PROFID –
## Script 4: Determinants models: performance, bootstrap validation & calibration
## Primary outcome: HF_BIN_eq3 (triple HF therapy = 3 classes)
## Aligns with Script 1 (HFrEF 2000–2020 dataset) and Script 3 (determinants)
## =============================================================================
suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(purrr)
  library(tibble)
  library(mice)
  library(pROC)
  library(ggplot2)
  library(ResourceSelection)
})
## =============================================================================
## 0) Small helpers
## =============================================================================
log_line <- function(...) cat(..., "\n")
logit_safe <- function(p, eps = 1e-6) {
  p <- pmin(pmax(p, eps), 1 - eps)
  log(p / (1 - p))
}
compute_auc_ci <- function(y, p_hat) {
  y <- as.integer(y)
  ok <- is.finite(p_hat) & !is.na(y)
  y <- y[ok]
  p_hat <- p_hat[ok]
  
  if (length(unique(y)) < 2) {
    return(tibble(auc = NA_real_, auc_low = NA_real_, auc_high = NA_real_))
  }
  
  roc_obj <- pROC::roc(response = y, predictor = p_hat,
                       quiet = TRUE, direction = "<")
  ci_auc <- pROC::ci.auc(roc_obj)
  tibble(
    auc = as.numeric(roc_obj$auc),
    auc_low = as.numeric(ci_auc[1]),
    auc_high = as.numeric(ci_auc[3])
  )
}

## AUC only (FAST) for bootstrap loops
compute_auc_only <- function(y, p_hat) {
  y <- as.integer(y)
  ok <- is.finite(p_hat) & !is.na(y)
  y <- y[ok]
  p_hat <- p_hat[ok]
  if (length(unique(y)) < 2) return(NA_real_)
  roc_obj <- pROC::roc(response = y, predictor = p_hat,
                       quiet = TRUE, direction = "<")
  as.numeric(roc_obj$auc)
}

safe_hl <- function(y, p_hat, g = 10) {
  y <- as.integer(y)
  ok <- is.finite(p_hat) & !is.na(y)
  y <- y[ok]
  p_hat <- p_hat[ok]
  
  n <- length(y)
  if (n < 20) return(NA_real_) # too small to be meaningful
  g_max <- min(g, max(2, floor(n / 5)))
  
  out <- try(ResourceSelection::hoslem.test(y, p_hat, g = g_max), silent = TRUE)
  if (inherits(out, "try-error")) return(NA_real_)
  out$p.value
}

make_formula <- function(outcome, predictors, data) {
  predictors <- predictors[predictors %in% names(data)]
  if (length(predictors) == 0) {
    as.formula(paste(outcome, "~ 1"))
  } else {
    as.formula(paste(outcome, "~", paste(predictors, collapse = " + ")))
  }
}

calibration_data <- function(y, p_hat, n_groups = 10) {
  tibble(y = as.integer(y), p_hat = p_hat) %>%
    dplyr::filter(!is.na(y), is.finite(p_hat)) %>%
    dplyr::mutate(group = dplyr::ntile(p_hat, n_groups)) %>%
    dplyr::group_by(group) %>%
    dplyr::summarise(
      n = dplyr::n(),
      obs = mean(y),
      pred = mean(p_hat),
      .groups = "drop"
    )
}

## CC calibration plot (was missing)
make_calibration_plot <- function(formula, data, outcome_name,
                                  title, subtitle, n_groups = 10) {
  vars_needed <- all.vars(formula)
  d_sub <- data[, vars_needed, drop = FALSE]
  d_cc <- d_sub[complete.cases(d_sub), , drop = FALSE]
  if (nrow(d_cc) < 50) stop("Too few complete cases for calibration plot.")
  
  fit <- glm(formula, data = d_cc, family = binomial)
  p_hat <- predict(fit, type = "response")
  y <- d_cc[[outcome_name]]
  
  calib_df <- calibration_data(y = y, p_hat = p_hat, n_groups = n_groups)
  
  ggplot(calib_df, aes(x = pred, y = obs)) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    geom_point(size = 2) +
    geom_line() +
    labs(
      title = title,
      subtitle = subtitle,
      x = "Mean predicted probability",
      y = "Observed proportion"
    ) +
    coord_equal(xlim = c(0, 1), ylim = c(0, 1)) +
    theme_minimal()
}

## MI calibration plot using MI-averaged predicted probabilities
make_calibration_plot_MI <- function(formula, imp, outcome_name,
                                     main_title, subtitle,
                                     n_groups = 10) {
  
  vars_needed <- all.vars(formula)
  stopifnot(outcome_name %in% names(imp$data))
  
  y <- imp$data[[outcome_name]]
  n <- length(y)
  m <- imp$m
  
  preds_mat <- matrix(NA_real_, nrow = n, ncol = m)
  
  for (i in 1:m) {
    dat_i <- mice::complete(imp, action = i)
    dat_i <- dat_i[, vars_needed, drop = FALSE]
    fit_i <- suppressWarnings(glm(formula, data = dat_i, family = binomial))
    preds_mat[, i] <- predict(fit_i, type = "response")
  }
  
  p_hat <- rowMeans(preds_mat, na.rm = TRUE)
  calib_df <- calibration_data(y = y, p_hat = p_hat, n_groups = n_groups)
  
  ggplot(calib_df, aes(x = pred, y = obs)) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    geom_point(size = 2) +
    geom_line() +
    labs(
      title = main_title,
      subtitle = subtitle,
      x = "Mean predicted probability (MI-averaged)",
      y = "Observed proportion"
    ) +
    coord_equal(xlim = c(0, 1), ylim = c(0, 1)) +
    theme_minimal()
}

## =============================================================================
## 1) Load analysis data (from Script 1)
## =============================================================================
IN_FILE_PRIMARY <- "T:/Study_5/Results_Study5/Script1_HFrEF_UPDATED/df_study5_hfref.rds"
IN_FILE_FALLBACK <- "S:/AG/f-dhzc-profid/Data Transfer to Charite/df_study5.rds"

if (file.exists(IN_FILE_PRIMARY)) {
  df <- readRDS(IN_FILE_PRIMARY)
  log_line("[INFO] Loaded:", IN_FILE_PRIMARY)
} else {
  stopifnot(file.exists(IN_FILE_FALLBACK))
  df <- readRDS(IN_FILE_FALLBACK)
  log_line("[INFO] Loaded (fallback):", IN_FILE_FALLBACK)
}

stopifnot(is.data.frame(df))
log_line("[INFO] Dimensions: n=", nrow(df), " | p=", ncol(df))

OUT_DIR <- "T:/Study_5/Results_Study5/Script4_Determinants_Performance"
if (!dir.exists(OUT_DIR)) dir.create(OUT_DIR, recursive = TRUE)
log_line("[INFO] Output folder:", OUT_DIR)

## =============================================================================
## 2) Outcome definition (aligned with Script 3)
## =============================================================================
outcome_name <- "HF_BIN_eq3"

if (!(outcome_name %in% names(df))) {
  if ("HF_n_classes" %in% names(df)) {
    df <- df %>% mutate(HF_BIN_eq3 = as.integer(HF_n_classes == 3))
    log_line("[INFO] Derived HF_BIN_eq3 from HF_n_classes == 3")
  } else if ("HF_GDMT_count4" %in% names(df)) {
    df <- df %>% mutate(HF_BIN_eq3 = as.integer(as.character(HF_GDMT_count4) == "3"))
    log_line("[INFO] Derived HF_BIN_eq3 from HF_GDMT_count4 == '3'")
  } else {
    stop("[ERROR] Cannot derive outcome: need HF_n_classes or HF_GDMT_count4.")
  }
}
df[[outcome_name]] <- as.integer(df[[outcome_name]])

log_line("[CHECK] Outcome distribution (HF_BIN_eq3):")
print(table(df[[outcome_name]], useNA = "ifany"))

## =============================================================================
## 3) Predictor blocks (same structure as Script 3)
## =============================================================================
if ("Year_index" %in% names(df)) df$Year_index <- suppressWarnings(as.integer(df$Year_index))

if ("Age_num" %in% names(df)) df$Age_num <- suppressWarnings(as.numeric(df$Age_num))
if (!("Age_num" %in% names(df)) && ("Age" %in% names(df))) {
  df$Age_num <- suppressWarnings(as.numeric(df$Age))
}

if ("LVEF_num" %in% names(df)) df$LVEF_num <- suppressWarnings(as.numeric(df$LVEF_num))
if ("Sex_BIN_Male" %in% names(df)) df$Sex_BIN_Male <- as.integer(df$Sex_BIN_Male)
if ("DB" %in% names(df)) df$DB <- factor(df$DB)

if ("Baseline_within40d" %in% names(df)) {
  tab_b40 <- table(df$Baseline_within40d, useNA = "ifany")
  log_line("[CHECK] Baseline_within40d distribution:")
  print(tab_b40)
}

temporal_vars <- c("Year_index")
demographic_vars <- c("Age_num", "Sex_BIN_Male", "DB")

clinical_vars <- c(
  "LVEF_num",
  "ICD_BIN_Yes",
  "Baseline_within40d",
  "Time_index_MI_CHD_log1p",
  "Diabetes_BIN_Yes", "Hypertension_BIN_Yes",
  "Smoking_BIN_Yes", "AF_atrial_flutter_BIN_Yes",
  "Stroke_TIA_BIN_Yes"
)

lab_vars_all <- c(
  "eGFR_log1p", "NTProBNP_log1p", "Haemoglobin_log1p",
  "Sodium_log1p", "Potassium_log1p"
)

procedure_vars <- c(
  "PCI_BIN_Yes", "CABG_BIN_Yes", "Revascularisation_acute_BIN_Yes"
)

keep_existing <- function(v) intersect(v, names(df))
temporal_vars <- keep_existing(temporal_vars)
demographic_vars <- keep_existing(demographic_vars)
clinical_vars <- keep_existing(clinical_vars)
lab_vars_all <- keep_existing(lab_vars_all)
procedure_vars <- keep_existing(procedure_vars)

if ("Baseline_within40d" %in% clinical_vars) {
  u <- unique(df$Baseline_within40d[!is.na(df$Baseline_within40d)])
  if (length(u) < 2) {
    clinical_vars <- setdiff(clinical_vars, "Baseline_within40d")
    log_line("[NOTE] Baseline_within40d is constant -> excluded from models.")
  }
}

lab_vars <- character(0)
labs_dropped <- character(0)
if (length(lab_vars_all) > 0) {
  na_prop <- sapply(lab_vars_all, function(v) mean(is.na(df[[v]])))
  lab_vars <- names(na_prop[na_prop < 0.80])
  labs_dropped <- names(na_prop[na_prop >= 0.80])
  log_line("[INFO] Labs kept (<80% missing):", paste(lab_vars, collapse = ", "))
  if (length(labs_dropped) > 0) {
    log_line("[INFO] Labs dropped (>=80% missing):", paste(labs_dropped, collapse = ", "))
  }
}

## =============================================================================
## 4) Model formulas (M1–M4)
## =============================================================================
formulas_cc <- list(
  M1_temporal = make_formula(outcome_name, temporal_vars, df),
  M2_temporal_demo = make_formula(outcome_name, c(temporal_vars, demographic_vars), df)
)

formulas_mi <- list(
  M3_temporal_demo_clinical = make_formula(outcome_name, c(temporal_vars, demographic_vars, clinical_vars), df),
  M4_full_extended = make_formula(outcome_name, c(temporal_vars, demographic_vars, clinical_vars, lab_vars, procedure_vars), df)
)

log_line("\n[INFO] CC formulas:")
print(formulas_cc)
log_line("\n[INFO] MI formulas:")
print(formulas_mi)

## =============================================================================
## 5) Apparent performance – complete-case
## =============================================================================
fit_cc_with_perf <- function(formula, data, model_name, outcome_name) {
  
  vars_needed <- all.vars(formula)
  d_sub <- data[, vars_needed, drop = FALSE]
  d_cc <- d_sub[complete.cases(d_sub), , drop = FALSE]
  
  if (nrow(d_cc) == 0) return(NULL)
  
  y <- d_cc[[outcome_name]]
  if (length(unique(y)) < 2) return(NULL)
  
  fit <- suppressWarnings(glm(formula, data = d_cc, family = binomial))
  p_hat <- predict(fit, type = "response")
  
  auc_tab <- compute_auc_ci(y, p_hat)
  logit_p <- logit_safe(p_hat)
  
  calib_int_fit <- suppressWarnings(glm(y ~ offset(logit_p), family = binomial))
  calib_intercept <- unname(coef(calib_int_fit)[1])
  
  calib_slope_fit <- suppressWarnings(glm(y ~ logit_p, family = binomial))
  calib_slope <- unname(coef(calib_slope_fit)["logit_p"])
  
  hl_p <- safe_hl(y, p_hat, g = 10)
  
  tibble(
    model = model_name,
    analysis = "complete_case",
    n_analysis = nrow(d_cc),
    n_events = sum(y == 1, na.rm = TRUE),
    n_nonevents = sum(y == 0, na.rm = TRUE),
    auc = auc_tab$auc,
    auc_low = auc_tab$auc_low,
    auc_high = auc_tab$auc_high,
    calib_intercept = calib_intercept,
    calib_slope = calib_slope,
    hl_pvalue = hl_p,
    AIC = AIC(fit),
    BIC = BIC(fit)
  )
}

log_line("\n[INFO] Apparent performance – CC models (M1–M2)")
perf_cc <- purrr::imap_dfr(formulas_cc, ~ fit_cc_with_perf(.x, df, .y, outcome_name))
print(perf_cc)

## =============================================================================
## 6) Multiple imputation (for M3/M4 predictors)
## =============================================================================
all_predictors_mi <- unique(c(temporal_vars, demographic_vars, clinical_vars, lab_vars, procedure_vars))
mi_vars <- unique(c(outcome_name, all_predictors_mi))
mi_vars <- mi_vars[mi_vars %in% names(df)]

df_mi <- df %>%
  dplyr::select(all_of(mi_vars)) %>%
  dplyr::filter(!is.na(.data[[outcome_name]]))

log_line("\n[INFO] MI dataset: n=", nrow(df_mi), " | p=", ncol(df_mi))

set.seed(20251224)
imp <- mice(
  df_mi,
  m = 20,
  maxit = 20,
  method = "pmm",
  printFlag = TRUE
)

## save MI object for later scripts
imp_rds <- file.path(OUT_DIR, "imp_study5_mice_script4.rds")
saveRDS(imp, imp_rds)
log_line("[INFO] Saved MI object:", imp_rds)

diag_file <- file.path(OUT_DIR, "study5_MI_traceplots_script4.pdf")
pdf(diag_file); plot(imp); dev.off()
log_line("[INFO] Saved MICE traceplots:", diag_file)

## =============================================================================
## 7) Apparent performance – MI models (mean predicted risk across imputations)
## =============================================================================
fit_mi_with_perf <- function(formula, imp, model_name, outcome_name) {
  
  vars_needed <- all.vars(formula)
  dat0 <- imp$data
  y <- dat0[[outcome_name]]
  
  if (length(unique(y[!is.na(y)])) < 2) return(NULL)
  
  m <- imp$m
  preds_mat <- matrix(NA_real_, nrow = nrow(dat0), ncol = m)
  
  for (i in seq_len(m)) {
    dat_i <- complete(imp, action = i)
    dat_i <- dat_i[, vars_needed, drop = FALSE]
    fit_i <- suppressWarnings(glm(formula, data = dat_i, family = binomial))
    preds_mat[, i] <- predict(fit_i, type = "response")
  }
  
  p_hat <- rowMeans(preds_mat, na.rm = TRUE)
  
  auc_tab <- compute_auc_ci(y, p_hat)
  logit_p <- logit_safe(p_hat)
  
  calib_int_fit <- suppressWarnings(glm(y ~ offset(logit_p), family = binomial))
  calib_intercept <- unname(coef(calib_int_fit)[1])
  
  calib_slope_fit <- suppressWarnings(glm(y ~ logit_p, family = binomial))
  calib_slope <- unname(coef(calib_slope_fit)["logit_p"])
  
  hl_p <- safe_hl(y, p_hat, g = 10)
  
  dat1 <- complete(imp, action = 1)
  dat1 <- dat1[, vars_needed, drop = FALSE]
  fit1 <- suppressWarnings(glm(formula, data = dat1, family = binomial))
  
  tibble(
    model = model_name,
    analysis = "multiple_imputation",
    n_analysis = length(y),
    n_events = sum(y == 1, na.rm = TRUE),
    n_nonevents = sum(y == 0, na.rm = TRUE),
    auc = auc_tab$auc,
    auc_low = auc_tab$auc_low,
    auc_high = auc_tab$auc_high,
    calib_intercept = calib_intercept,
    calib_slope = calib_slope,
    hl_pvalue = hl_p,
    AIC = AIC(fit1),
    BIC = BIC(fit1)
  )
}

log_line("\n[INFO] Apparent performance – MI models (M3–M4)")
perf_mi <- purrr::imap_dfr(formulas_mi, ~ fit_mi_with_perf(.x, imp, .y, outcome_name))
print(perf_mi)

perf_all <- dplyr::bind_rows(perf_cc, perf_mi)

saveRDS(perf_all, file.path(OUT_DIR, "study5_model_performance_summary.rds"))
write.csv(perf_all, file.path(OUT_DIR, "study5_model_performance_summary.csv"), row.names = FALSE)
log_line("[OK] Saved apparent performance summary in:", OUT_DIR)

## =============================================================================
## 8) Bootstrap internal validation (AUC optimism correction)
## NOTE: Done on first completed dataset (internal validation only)
## =============================================================================
bootstrap_validate_auc <- function(formula, data, outcome_name, model_name,
                                   B = 1000, seed = 20251225,
                                   checkpoint_every = 50,
                                   checkpoint_file = NULL) {
  
  set.seed(seed)
  vars_needed <- all.vars(formula)
  d <- data[, vars_needed, drop = FALSE]
  d <- d[complete.cases(d), , drop = FALSE]
  
  y <- d[[outcome_name]]
  if (length(unique(y)) < 2) return(NULL)
  
  fit_app <- suppressWarnings(glm(formula, data = d, family = binomial))
  p_app <- predict(fit_app, type = "response")
  auc_app <- compute_auc_only(y, p_app)
  
  optimism <- rep(NA_real_, B)
  
  for (b in seq_len(B)) {
    idx <- sample(seq_len(nrow(d)), replace = TRUE)
    d_boot <- d[idx, , drop = FALSE]
    
    fit_boot <- try(suppressWarnings(glm(formula, data = d_boot, family = binomial)), silent = TRUE)
    if (inherits(fit_boot, "try-error")) {
      optimism[b] <- NA_real_
    } else {
      p_boot_fit <- predict(fit_boot, type = "response")
      auc_boot_fit <- compute_auc_only(d_boot[[outcome_name]], p_boot_fit)
      
      p_boot_orig <- predict(fit_boot, newdata = d, type = "response")
      auc_boot_orig <- compute_auc_only(y, p_boot_orig)
      
      optimism[b] <- auc_boot_fit - auc_boot_orig
    }
    
    if (!is.null(checkpoint_file) && (b %% checkpoint_every == 0)) {
      saveRDS(list(model = model_name, b_done = b, optimism = optimism, auc_app = auc_app),
              checkpoint_file)
      log_line("[INFO] Checkpoint saved:", checkpoint_file, "| b=", b)
    }
  }
  
  opt <- optimism[is.finite(optimism)]
  mean_opt <- mean(opt)
  auc_corr <- auc_app - mean_opt
  
  tibble(
    model = model_name,
    B_bootstrap = B,
    auc_apparent = auc_app,
    auc_optimism_mean = mean_opt,
    auc_corrected = auc_corr,
    n_failed = sum(!is.finite(optimism))
  )
}

log_line("\n[INFO] Bootstrap validation (AUC) – models M3 and M4 (dat1)")
dat1 <- complete(imp, action = 1)

ck_M3 <- file.path(OUT_DIR, "checkpoint_boot_M3.rds")
ck_M4 <- file.path(OUT_DIR, "checkpoint_boot_M4.rds")

boot_M3 <- bootstrap_validate_auc(
  formula = formulas_mi$M3_temporal_demo_clinical,
  data = dat1,
  outcome_name = outcome_name,
  model_name = "M3_temporal_demo_clinical",
  B = 1000,
  checkpoint_every = 50,
  checkpoint_file = ck_M3
)

boot_M4 <- bootstrap_validate_auc(
  formula = formulas_mi$M4_full_extended,
  data = dat1,
  outcome_name = outcome_name,
  model_name = "M4_full_extended",
  B = 1000,
  checkpoint_every = 50,
  checkpoint_file = ck_M4
)

boot_results <- dplyr::bind_rows(boot_M3, boot_M4)
print(boot_results)

saveRDS(boot_results, file.path(OUT_DIR, "study5_bootstrap_auc_validation.rds"))
write.csv(boot_results, file.path(OUT_DIR, "study5_bootstrap_auc_validation.csv"), row.names = FALSE)
log_line("[OK] Saved bootstrap validation in:", OUT_DIR)

## =============================================================================
## 9) Calibration plots (M2, M3, M4)
## =============================================================================
p_cal_M2 <- make_calibration_plot(
  formula = formulas_cc$M2_temporal_demo,
  data = df,
  outcome_name = outcome_name,
  title = "Calibration – HF triple therapy model",
  subtitle = "M2: Temporal + demographics (complete-case)"
)

p_cal_M3 <- make_calibration_plot_MI(
  formula = formulas_mi$M3_temporal_demo_clinical,
  imp = imp,
  outcome_name = outcome_name,
  main_title = "Calibration – HF triple therapy model",
  subtitle = "M3: Temporal + demographics + clinical (MI-averaged predictions)"
)

p_cal_M4 <- make_calibration_plot_MI(
  formula = formulas_mi$M4_full_extended,
  imp = imp,
  outcome_name = outcome_name,
  main_title = "Calibration – HF triple therapy model",
  subtitle = "M4: Full extended (MI-averaged predictions)"
)

png_M2 <- file.path(OUT_DIR, "calibration_M2.png")
png_M3 <- file.path(OUT_DIR, "calibration_M3.png")
png_M4 <- file.path(OUT_DIR, "calibration_M4.png")

png(png_M2, width = 7, height = 6, units = "in", res = 300); print(p_cal_M2); dev.off()
png(png_M3, width = 7, height = 6, units = "in", res = 300); print(p_cal_M3); dev.off()
png(png_M4, width = 7, height = 6, units = "in", res = 300); print(p_cal_M4); dev.off()

log_line("[OK] Saved calibration plots:", png_M2, "|", png_M3, "|", png_M4)

log_line("\n[DONE] Script 4 completed successfully.")

# =================================================================================

# =======================================================================================
# Study 5 – Script 4 (UPDATED, ENGLISH)
# Determinants models: performance, bootstrap validation & calibration
# Primary outcome: HF_BIN_eq3 (triple HF therapy = 3 classes)
# UPDATE: Adds sensitivity outcome HF_BIN_geq2 (>=2 classes) as PASSIVE in MICE
# via a standardized count variable HF_count_for_MI derived from:
# - HF_n_classes OR HF_GDMT_count4 (whichever exists)
#
# Outputs:
# - imp_study5_mice_script4.rds (includes HF_count_for_MI and HF_BIN_geq2 if derivable)
# - study5_model_performance_summary.csv/.rds
# - study5_bootstrap_auc_validation.csv/.rds
# - calibration_M2.png, calibration_M3.png, calibration_M4.png
# - study5_MI_traceplots_script4.pdf
# =======================================================================================

suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(purrr)
  library(tibble)
  library(mice)
  library(pROC)
  library(ggplot2)
  library(ResourceSelection)
})

# =============================================================================
# 0) Helpers
# =============================================================================
log_line <- function(...) cat(..., "\n")

logit_safe <- function(p, eps = 1e-6) {
  p <- pmin(pmax(p, eps), 1 - eps)
  log(p / (1 - p))
}

compute_auc_ci <- function(y, p_hat) {
  y <- as.integer(y)
  ok <- is.finite(p_hat) & !is.na(y)
  y <- y[ok]
  p_hat <- p_hat[ok]
  
  if (length(unique(y)) < 2) {
    return(tibble(auc = NA_real_, auc_low = NA_real_, auc_high = NA_real_))
  }
  
  roc_obj <- pROC::roc(response = y, predictor = p_hat, quiet = TRUE, direction = "<")
  ci_auc <- pROC::ci.auc(roc_obj)
  
  tibble(
    auc = as.numeric(roc_obj$auc),
    auc_low = as.numeric(ci_auc[1]),
    auc_high = as.numeric(ci_auc[3])
  )
}

compute_auc_only <- function(y, p_hat) {
  y <- as.integer(y)
  ok <- is.finite(p_hat) & !is.na(y)
  y <- y[ok]
  p_hat <- p_hat[ok]
  if (length(unique(y)) < 2) return(NA_real_)
  roc_obj <- pROC::roc(response = y, predictor = p_hat, quiet = TRUE, direction = "<")
  as.numeric(roc_obj$auc)
}

safe_hl <- function(y, p_hat, g = 10) {
  y <- as.integer(y)
  ok <- is.finite(p_hat) & !is.na(y)
  y <- y[ok]
  p_hat <- p_hat[ok]
  
  n <- length(y)
  if (n < 20) return(NA_real_)
  g_max <- min(g, max(2, floor(n / 5)))
  
  out <- try(ResourceSelection::hoslem.test(y, p_hat, g = g_max), silent = TRUE)
  if (inherits(out, "try-error")) return(NA_real_)
  out$p.value
}

make_formula <- function(outcome, predictors, data) {
  predictors <- predictors[predictors %in% names(data)]
  if (length(predictors) == 0) {
    as.formula(paste(outcome, "~ 1"))
  } else {
    as.formula(paste(outcome, "~", paste(predictors, collapse = " + ")))
  }
}

calibration_data <- function(y, p_hat, n_groups = 10) {
  tibble(y = as.integer(y), p_hat = p_hat) %>%
    dplyr::filter(!is.na(y), is.finite(p_hat)) %>%
    dplyr::mutate(group = dplyr::ntile(p_hat, n_groups)) %>%
    dplyr::group_by(group) %>%
    dplyr::summarise(
      n = dplyr::n(),
      obs = mean(y),
      pred = mean(p_hat),
      .groups = "drop"
    )
}

make_calibration_plot <- function(formula, data, outcome_name,
                                  title, subtitle, n_groups = 10) {
  vars_needed <- all.vars(formula)
  d_sub <- data[, vars_needed, drop = FALSE]
  d_cc <- d_sub[complete.cases(d_sub), , drop = FALSE]
  if (nrow(d_cc) < 50) stop("Too few complete cases for calibration plot.")
  
  fit <- glm(formula, data = d_cc, family = binomial)
  p_hat <- predict(fit, type = "response")
  y <- d_cc[[outcome_name]]
  
  calib_df <- calibration_data(y = y, p_hat = p_hat, n_groups = n_groups)
  
  ggplot(calib_df, aes(x = pred, y = obs)) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    geom_point(size = 2) +
    geom_line() +
    labs(
      title = title,
      subtitle = subtitle,
      x = "Mean predicted probability",
      y = "Observed proportion"
    ) +
    coord_equal(xlim = c(0, 1), ylim = c(0, 1)) +
    theme_minimal()
}

make_calibration_plot_MI <- function(formula, imp, outcome_name,
                                     main_title, subtitle,
                                     n_groups = 10) {
  
  vars_needed <- all.vars(formula)
  stopifnot(outcome_name %in% names(imp$data))
  
  y <- imp$data[[outcome_name]]
  n <- length(y)
  m <- imp$m
  
  preds_mat <- matrix(NA_real_, nrow = n, ncol = m)
  
  for (i in seq_len(m)) {
    dat_i <- mice::complete(imp, action = i)
    dat_i <- dat_i[, vars_needed, drop = FALSE]
    fit_i <- suppressWarnings(glm(formula, data = dat_i, family = binomial))
    preds_mat[, i] <- predict(fit_i, type = "response")
  }
  
  p_hat <- rowMeans(preds_mat, na.rm = TRUE)
  calib_df <- calibration_data(y = y, p_hat = p_hat, n_groups = n_groups)
  
  ggplot(calib_df, aes(x = pred, y = obs)) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    geom_point(size = 2) +
    geom_line() +
    labs(
      title = main_title,
      subtitle = subtitle,
      x = "Mean predicted probability (MI-averaged)",
      y = "Observed proportion"
    ) +
    coord_equal(xlim = c(0, 1), ylim = c(0, 1)) +
    theme_minimal()
}

# =============================================================================
# 1) Load analysis data (from Script 1)
# =============================================================================
IN_FILE_PRIMARY <- "T:/Study_5/Results_Study5/Script1_HFrEF_UPDATED/df_study5_hfref.rds"
IN_FILE_FALLBACK <- "S:/AG/f-dhzc-profid/Data Transfer to Charite/df_study5.rds"

if (file.exists(IN_FILE_PRIMARY)) {
  df <- readRDS(IN_FILE_PRIMARY)
  log_line("[INFO] Loaded:", IN_FILE_PRIMARY)
} else {
  stopifnot(file.exists(IN_FILE_FALLBACK))
  df <- readRDS(IN_FILE_FALLBACK)
  log_line("[INFO] Loaded (fallback):", IN_FILE_FALLBACK)
}

stopifnot(is.data.frame(df))
log_line("[INFO] Dimensions: n=", nrow(df), " | p=", ncol(df))

OUT_DIR <- "T:/Study_5/Results_Study5/Script4_Determinants_Performance"
if (!dir.exists(OUT_DIR)) dir.create(OUT_DIR, recursive = TRUE)
log_line("[INFO] Output folder:", OUT_DIR)

# =============================================================================
# 2) Outcomes: HF_BIN_eq3 (primary) + HF_count_for_MI + HF_BIN_geq2 (sensitivity)
# =============================================================================
outcome_name <- "HF_BIN_eq3"

# Ensure HF_BIN_eq3 exists
if (!(outcome_name %in% names(df))) {
  if ("HF_n_classes" %in% names(df)) {
    df <- df %>% mutate(HF_BIN_eq3 = as.integer(HF_n_classes == 3))
    log_line("[INFO] Derived HF_BIN_eq3 from HF_n_classes == 3")
  } else if ("HF_GDMT_count4" %in% names(df)) {
    df <- df %>% mutate(HF_BIN_eq3 = as.integer(as.character(HF_GDMT_count4) == "3"))
    log_line("[INFO] Derived HF_BIN_eq3 from HF_GDMT_count4 == '3'")
  } else {
    stop("[ERROR] Cannot derive outcome: need HF_n_classes or HF_GDMT_count4.")
  }
}
df[[outcome_name]] <- as.integer(df[[outcome_name]])

log_line("[CHECK] Outcome distribution (HF_BIN_eq3):")
print(table(df[[outcome_name]], useNA = "ifany"))

# NEW: Standardized HF count + sensitivity outcome
hf_count_src <- NA_character_

if ("HF_n_classes" %in% names(df)) {
  hf_count_src <- "HF_n_classes"
  df$HF_n_classes <- suppressWarnings(as.numeric(df$HF_n_classes))
} else if ("HF_GDMT_count4" %in% names(df)) {
  hf_count_src <- "HF_GDMT_count4"
  df$HF_GDMT_count4 <- suppressWarnings(as.numeric(as.character(df$HF_GDMT_count4)))
}

if (!is.na(hf_count_src)) {
  df$HF_count_for_MI <- df[[hf_count_src]]
  df$HF_BIN_geq2 <- ifelse(
    is.na(df$HF_count_for_MI),
    NA_integer_,
    as.integer(df$HF_count_for_MI >= 2)
  )
  log_line("[INFO] Added HF_count_for_MI + HF_BIN_geq2 using:", hf_count_src)
  log_line("[CHECK] Sensitivity outcome distribution (HF_BIN_geq2):")
  print(table(df$HF_BIN_geq2, useNA = "ifany"))
} else {
  log_line("[WARN] No HF_n_classes / HF_GDMT_count4 found -> HF_BIN_geq2 cannot be created here.")
}

# =============================================================================
# 3) Predictor blocks (same structure as Script 3)
# =============================================================================
if ("Year_index" %in% names(df)) df$Year_index <- suppressWarnings(as.integer(df$Year_index))

if ("Age_num" %in% names(df)) df$Age_num <- suppressWarnings(as.numeric(df$Age_num))
if (!("Age_num" %in% names(df)) && ("Age" %in% names(df))) {
  df$Age_num <- suppressWarnings(as.numeric(df$Age))
}

if ("LVEF_num" %in% names(df)) df$LVEF_num <- suppressWarnings(as.numeric(df$LVEF_num))
if ("Sex_BIN_Male" %in% names(df)) df$Sex_BIN_Male <- as.integer(df$Sex_BIN_Male)
if ("DB" %in% names(df)) df$DB <- factor(df$DB)

if ("Baseline_within40d" %in% names(df)) {
  tab_b40 <- table(df$Baseline_within40d, useNA = "ifany")
  log_line("[CHECK] Baseline_within40d distribution:")
  print(tab_b40)
}

temporal_vars <- c("Year_index")
demographic_vars <- c("Age_num", "Sex_BIN_Male", "DB")

clinical_vars <- c(
  "LVEF_num",
  "ICD_BIN_Yes",
  "Baseline_within40d",
  "Time_index_MI_CHD_log1p",
  "Diabetes_BIN_Yes", "Hypertension_BIN_Yes",
  "Smoking_BIN_Yes", "AF_atrial_flutter_BIN_Yes",
  "Stroke_TIA_BIN_Yes"
)

lab_vars_all <- c(
  "eGFR_log1p", "NTProBNP_log1p", "Haemoglobin_log1p",
  "Sodium_log1p", "Potassium_log1p"
)

procedure_vars <- c(
  "PCI_BIN_Yes", "CABG_BIN_Yes", "Revascularisation_acute_BIN_Yes"
)

keep_existing <- function(v) intersect(v, names(df))
temporal_vars <- keep_existing(temporal_vars)
demographic_vars <- keep_existing(demographic_vars)
clinical_vars <- keep_existing(clinical_vars)
lab_vars_all <- keep_existing(lab_vars_all)
procedure_vars <- keep_existing(procedure_vars)

if ("Baseline_within40d" %in% clinical_vars) {
  u <- unique(df$Baseline_within40d[!is.na(df$Baseline_within40d)])
  if (length(u) < 2) {
    clinical_vars <- setdiff(clinical_vars, "Baseline_within40d")
    log_line("[NOTE] Baseline_within40d is constant -> excluded from models.")
  }
}

lab_vars <- character(0)
labs_dropped <- character(0)
if (length(lab_vars_all) > 0) {
  na_prop <- sapply(lab_vars_all, function(v) mean(is.na(df[[v]])))
  lab_vars <- names(na_prop[na_prop < 0.80])
  labs_dropped <- names(na_prop[na_prop >= 0.80])
  log_line("[INFO] Labs kept (<80% missing):", paste(lab_vars, collapse = ", "))
  if (length(labs_dropped) > 0) {
    log_line("[INFO] Labs dropped (>=80% missing):", paste(labs_dropped, collapse = ", "))
  }
}

# =============================================================================
# 4) Model formulas (M1–M4)
# =============================================================================
formulas_cc <- list(
  M1_temporal = make_formula(outcome_name, temporal_vars, df),
  M2_temporal_demo = make_formula(outcome_name, c(temporal_vars, demographic_vars), df)
)

formulas_mi <- list(
  M3_temporal_demo_clinical = make_formula(outcome_name, c(temporal_vars, demographic_vars, clinical_vars), df),
  M4_full_extended = make_formula(outcome_name, c(temporal_vars, demographic_vars, clinical_vars, lab_vars, procedure_vars), df)
)

log_line("\n[INFO] CC formulas:")
print(formulas_cc)
log_line("\n[INFO] MI formulas:")
print(formulas_mi)

# =============================================================================
# 5) Apparent performance – complete-case
# =============================================================================
fit_cc_with_perf <- function(formula, data, model_name, outcome_name) {
  
  vars_needed <- all.vars(formula)
  d_sub <- data[, vars_needed, drop = FALSE]
  d_cc <- d_sub[complete.cases(d_sub), , drop = FALSE]
  
  if (nrow(d_cc) == 0) return(NULL)
  
  y <- d_cc[[outcome_name]]
  if (length(unique(y)) < 2) return(NULL)
  
  fit <- suppressWarnings(glm(formula, data = d_cc, family = binomial))
  p_hat <- predict(fit, type = "response")
  
  auc_tab <- compute_auc_ci(y, p_hat)
  logit_p <- logit_safe(p_hat)
  
  calib_int_fit <- suppressWarnings(glm(y ~ offset(logit_p), family = binomial))
  calib_intercept <- unname(coef(calib_int_fit)[1])
  
  calib_slope_fit <- suppressWarnings(glm(y ~ logit_p, family = binomial))
  calib_slope <- unname(coef(calib_slope_fit)["logit_p"])
  
  hl_p <- safe_hl(y, p_hat, g = 10)
  
  tibble(
    model = model_name,
    analysis = "complete_case",
    n_analysis = nrow(d_cc),
    n_events = sum(y == 1, na.rm = TRUE),
    n_nonevents = sum(y == 0, na.rm = TRUE),
    auc = auc_tab$auc,
    auc_low = auc_tab$auc_low,
    auc_high = auc_tab$auc_high,
    calib_intercept = calib_intercept,
    calib_slope = calib_slope,
    hl_pvalue = hl_p,
    AIC = AIC(fit),
    BIC = BIC(fit)
  )
}

log_line("\n[INFO] Apparent performance – CC models (M1–M2)")
perf_cc <- purrr::imap_dfr(formulas_cc, ~ fit_cc_with_perf(.x, df, .y, outcome_name))
print(perf_cc)

# =============================================================================
# 6) Multiple imputation (for M3/M4 predictors)
# UPDATE: include HF_count_for_MI + HF_BIN_geq2 so they are saved into imp
# and define HF_BIN_geq2 as PASSIVE: ~I(HF_count_for_MI >= 2)
# =============================================================================
all_predictors_mi <- unique(c(temporal_vars, demographic_vars, clinical_vars, lab_vars, procedure_vars))

mi_vars <- unique(c(outcome_name, all_predictors_mi, "HF_count_for_MI", "HF_BIN_geq2"))
mi_vars <- mi_vars[mi_vars %in% names(df)]

df_mi <- df %>%
  dplyr::select(all_of(mi_vars)) %>%
  dplyr::filter(!is.na(.data[[outcome_name]]))

log_line("\n[INFO] MI dataset: n=", nrow(df_mi), " | p=", ncol(df_mi))
log_line("[INFO] MI variables: ", paste(names(df_mi), collapse = ", "))

meth <- mice::make.method(df_mi)

# Passive sensitivity outcome
if ("HF_BIN_geq2" %in% names(df_mi) && "HF_count_for_MI" %in% names(df_mi)) {
  meth["HF_BIN_geq2"] <- "~I(HF_count_for_MI >= 2)"
}

# Ensure count is imputed (if present)
if ("HF_count_for_MI" %in% names(df_mi)) {
  meth["HF_count_for_MI"] <- "pmm"
}

set.seed(20251224)
imp <- mice(
  df_mi,
  m = 20,
  maxit = 20,
  method = meth,
  printFlag = TRUE
)

# Sanity checks for downstream Script5B
log_line("[CHECK] imp$data has HF_count_for_MI? ", "HF_count_for_MI" %in% names(imp$data))
log_line("[CHECK] imp$data has HF_BIN_geq2? ", "HF_BIN_geq2" %in% names(imp$data))

imp_rds <- file.path(OUT_DIR, "imp_study5_mice_script4.rds")
saveRDS(imp, imp_rds)
log_line("[INFO] Saved MI object:", imp_rds)

diag_file <- file.path(OUT_DIR, "study5_MI_traceplots_script4.pdf")
pdf(diag_file); plot(imp); dev.off()
log_line("[INFO] Saved MICE traceplots:", diag_file)

# =============================================================================
# 7) Apparent performance – MI models (mean predicted risk across imputations)
# =============================================================================
fit_mi_with_perf <- function(formula, imp, model_name, outcome_name) {
  
  vars_needed <- all.vars(formula)
  dat0 <- imp$data
  y <- dat0[[outcome_name]]
  
  if (length(unique(y[!is.na(y)])) < 2) return(NULL)
  
  m <- imp$m
  preds_mat <- matrix(NA_real_, nrow = nrow(dat0), ncol = m)
  
  for (i in seq_len(m)) {
    dat_i <- complete(imp, action = i)
    dat_i <- dat_i[, vars_needed, drop = FALSE]
    fit_i <- suppressWarnings(glm(formula, data = dat_i, family = binomial))
    preds_mat[, i] <- predict(fit_i, type = "response")
  }
  
  p_hat <- rowMeans(preds_mat, na.rm = TRUE)
  
  auc_tab <- compute_auc_ci(y, p_hat)
  logit_p <- logit_safe(p_hat)
  
  calib_int_fit <- suppressWarnings(glm(y ~ offset(logit_p), family = binomial))
  calib_intercept <- unname(coef(calib_int_fit)[1])
  
  calib_slope_fit <- suppressWarnings(glm(y ~ logit_p, family = binomial))
  calib_slope <- unname(coef(calib_slope_fit)["logit_p"])
  
  hl_p <- safe_hl(y, p_hat, g = 10)
  
  dat1 <- complete(imp, action = 1)
  dat1 <- dat1[, vars_needed, drop = FALSE]
  fit1 <- suppressWarnings(glm(formula, data = dat1, family = binomial))
  
  tibble(
    model = model_name,
    analysis = "multiple_imputation",
    n_analysis = length(y),
    n_events = sum(y == 1, na.rm = TRUE),
    n_nonevents = sum(y == 0, na.rm = TRUE),
    auc = auc_tab$auc,
    auc_low = auc_tab$auc_low,
    auc_high = auc_tab$auc_high,
    calib_intercept = calib_intercept,
    calib_slope = calib_slope,
    hl_pvalue = hl_p,
    AIC = AIC(fit1),
    BIC = BIC(fit1)
  )
}

log_line("\n[INFO] Apparent performance – MI models (M3–M4)")
perf_mi <- purrr::imap_dfr(formulas_mi, ~ fit_mi_with_perf(.x, imp, .y, outcome_name))
print(perf_mi)

perf_all <- dplyr::bind_rows(perf_cc, perf_mi)

saveRDS(perf_all, file.path(OUT_DIR, "study5_model_performance_summary.rds"))
write.csv(perf_all, file.path(OUT_DIR, "study5_model_performance_summary.csv"), row.names = FALSE)
log_line("[OK] Saved apparent performance summary in:", OUT_DIR)

# =============================================================================
# 8) Bootstrap internal validation (AUC optimism correction)
# NOTE: Done on first completed dataset (internal validation only)
# =============================================================================
bootstrap_validate_auc <- function(formula, data, outcome_name, model_name,
                                   B = 1000, seed = 20251225,
                                   checkpoint_every = 50,
                                   checkpoint_file = NULL) {
  
  set.seed(seed)
  vars_needed <- all.vars(formula)
  d <- data[, vars_needed, drop = FALSE]
  d <- d[complete.cases(d), , drop = FALSE]
  
  y <- d[[outcome_name]]
  if (length(unique(y)) < 2) return(NULL)
  
  fit_app <- suppressWarnings(glm(formula, data = d, family = binomial))
  p_app <- predict(fit_app, type = "response")
  auc_app <- compute_auc_only(y, p_app)
  
  optimism <- rep(NA_real_, B)
  
  for (b in seq_len(B)) {
    idx <- sample(seq_len(nrow(d)), replace = TRUE)
    d_boot <- d[idx, , drop = FALSE]
    
    fit_boot <- try(suppressWarnings(glm(formula, data = d_boot, family = binomial)), silent = TRUE)
    if (inherits(fit_boot, "try-error")) {
      optimism[b] <- NA_real_
    } else {
      p_boot_fit <- predict(fit_boot, type = "response")
      auc_boot_fit <- compute_auc_only(d_boot[[outcome_name]], p_boot_fit)
      
      p_boot_orig <- predict(fit_boot, newdata = d, type = "response")
      auc_boot_orig <- compute_auc_only(y, p_boot_orig)
      
      optimism[b] <- auc_boot_fit - auc_boot_orig
    }
    
    if (!is.null(checkpoint_file) && (b %% checkpoint_every == 0)) {
      saveRDS(list(model = model_name, b_done = b, optimism = optimism, auc_app = auc_app),
              checkpoint_file)
      log_line("[INFO] Checkpoint saved:", checkpoint_file, "| b=", b)
    }
  }
  
  opt <- optimism[is.finite(optimism)]
  mean_opt <- mean(opt)
  auc_corr <- auc_app - mean_opt
  
  tibble(
    model = model_name,
    B_bootstrap = B,
    auc_apparent = auc_app,
    auc_optimism_mean = mean_opt,
    auc_corrected = auc_corr,
    n_failed = sum(!is.finite(optimism))
  )
}

log_line("\n[INFO] Bootstrap validation (AUC) – models M3 and M4 (dat1)")
dat1 <- complete(imp, action = 1)

ck_M3 <- file.path(OUT_DIR, "checkpoint_boot_M3.rds")
ck_M4 <- file.path(OUT_DIR, "checkpoint_boot_M4.rds")

boot_M3 <- bootstrap_validate_auc(
  formula = formulas_mi$M3_temporal_demo_clinical,
  data = dat1,
  outcome_name = outcome_name,
  model_name = "M3_temporal_demo_clinical",
  B = 1000,
  checkpoint_every = 50,
  checkpoint_file = ck_M3
)

boot_M4 <- bootstrap_validate_auc(
  formula = formulas_mi$M4_full_extended,
  data = dat1,
  outcome_name = outcome_name,
  model_name = "M4_full_extended",
  B = 1000,
  checkpoint_every = 50,
  checkpoint_file = ck_M4
)

boot_results <- dplyr::bind_rows(boot_M3, boot_M4)
print(boot_results)

saveRDS(boot_results, file.path(OUT_DIR, "study5_bootstrap_auc_validation.rds"))
write.csv(boot_results, file.path(OUT_DIR, "study5_bootstrap_auc_validation.csv"), row.names = FALSE)
log_line("[OK] Saved bootstrap validation in:", OUT_DIR)

# =============================================================================
# 9) Calibration plots (M2, M3, M4)
# =============================================================================
p_cal_M2 <- make_calibration_plot(
  formula = formulas_cc$M2_temporal_demo,
  data = df,
  outcome_name = outcome_name,
  title = "Calibration – HF triple therapy model",
  subtitle = "M2: Temporal + demographics (complete-case)"
)

p_cal_M3 <- make_calibration_plot_MI(
  formula = formulas_mi$M3_temporal_demo_clinical,
  imp = imp,
  outcome_name = outcome_name,
  main_title = "Calibration – HF triple therapy model",
  subtitle = "M3: Temporal + demographics + clinical (MI-averaged predictions)"
)

p_cal_M4 <- make_calibration_plot_MI(
  formula = formulas_mi$M4_full_extended,
  imp = imp,
  outcome_name = outcome_name,
  main_title = "Calibration – HF triple therapy model",
  subtitle = "M4: Full extended (MI-averaged predictions)"
)

png_M2 <- file.path(OUT_DIR, "calibration_M2.png")
png_M3 <- file.path(OUT_DIR, "calibration_M3.png")
png_M4 <- file.path(OUT_DIR, "calibration_M4.png")

png(png_M2, width = 7, height = 6, units = "in", res = 300); print(p_cal_M2); dev.off()
png(png_M3, width = 7, height = 6, units = "in", res = 300); print(p_cal_M3); dev.off()
png(png_M4, width = 7, height = 6, units = "in", res = 300); print(p_cal_M4); dev.off()

log_line("[OK] Saved calibration plots:", png_M2, "|", png_M3, "|", png_M4)

log_line("\n[DONE] Script 4 completed successfully.")
# =======================================================================================


